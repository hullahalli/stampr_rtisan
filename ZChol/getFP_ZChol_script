getFP <- function(ReadsTableName, CFUtable, WhereAreReferences, minweight, outputfilename) {
  
  ReadsTable <- read.csv(ReadsTableName, row.names = 1)
  
  
  if(!is.null(CFUtable)) {
    CFUtable <- read.csv(CFUtable)
  }
  
  ReferenceVector <- rowMeans(cbind(ReadsTable[,WhereAreReferences]))
  TestNames <- colnames(ReadsTable)
  TableWithoutNoise <- data.frame(row.names = rownames(ReadsTable))
  
  
  
  ResiliencyIndices <- function(samplename, plots = FALSE){
    
    invec <- ReferenceVector
    #invec <- rowSums(ReadsTable)
    outvec <- read.csv(ReadsTableName, row.names = 1)[,which(TestNames == samplename)]
    TotalReads <- sum(outvec)
    
    if(!is.null(CFUtable)) {
      cfu <<- CFUtable[CFUtable[,1]==samplename,2]
    } else {cfu <- 1E8}
    cfu <- as.numeric(cfu)


    oneratio <- sqrt( sum(outvec > 1) / sum(outvec == 1))
    if(oneratio < 1) {oneratio <- -(oneratio ^2)}

    NoiseCorrectFactor <- 10^-oneratio
    NoiseDist <- rowSums(ReadsTable)
    # BarcodesToInclude <- which(outvec != 0)
    # NoiseDist[-BarcodesToInclude] <- 0

    ResampledInvec <- rmultinom(50, round(sum(outvec) * NoiseCorrectFactor), NoiseDist)
    outvecmultiple <- rep(outvec,50)
    dim(outvecmultiple) <- dim(ResampledInvec)
    outvec <- ceiling(rowMeans(outvec - ResampledInvec))
    outvec[outvec < 0] <- 0
    
    if ( sum(outvec > 1) >  2*sum(outvec == 1) ) 
    {times <- min(ceiling(cfu), sum(outvec > 0))
    } else  {times <- min(ceiling(cfu), sum(outvec > 1))}
    
    greatestdif <- NULL
    if(sum(outvec == 1) > 1.2* sum(outvec > 1)) {
      outvec <- outvec - 1
      outveccopy <- outvec
      outveccopy[outveccopy < 2] <- 0
      outvecsorted <- as.numeric(sort(outveccopy, decreasing = TRUE))
      plotdif <- c(outvecsorted[2:length(outvecsorted)], tail(outvecsorted, 1))
      plotsub <- na.omit(log(outvecsorted) - log(plotdif))
      plotsub <- plotsub[1:length(plotsub)-1]
      greatestdif <- which(plotsub==max(plotsub))
      }

    
#    if ( sum(outvec > 1) >  sum(outvec == 1) ) 
#    {times <- min(ceiling(cfu), sum(outvec > 0))
#    } else  {
#      ResampledInvec <- rmultinom(50, round(sum(outvec)), round (rowSums(ReadsTable)))
#      outvecmultiple <- rep(outvec,50)
#      dim(outvecmultiple) <- dim(ResampledInvec)
#      outvec <- ceiling(rowMeans(outvec - ResampledInvec))
#      outvec[outvec < 0] <- 0
      # times <- min(ceiling(cfu), sum(outvec > 1))
#      }
    
   
    bind <- cbind(invec, outvec)
    
    bindsorted <- bind[order(bind[,2]),]
    
    x <- 0
    
    minusone <- function(dims) {
      input <- bindsorted[,1][1:dims]
      out <- bindsorted[,2][1:dims]
      inputprop <- na.omit(input/sum(input))
      outprop <- na.omit(out / sum(out))
      num <- ( outprop - inputprop ) ^ 2
      den <- inputprop*(1-inputprop)
      sigma <- num / den
      sigma <- sigma[sigma!=Inf]
      F <- mean(na.omit(sigma))
      1 / (F - 1/sum(invec) - 1/sum(outvec)) 
    }
    
    timestorep <- rev(seq(from = length(invec) - times + 1, to = length(invec), by = 1))
    x <- sapply(timestorep, minusone)
    
    scanformin <- function(p) {
      start <- p
      findmin <- function() {
        where <- which(x == start)
        #newlocation <- abs(rnorm(1, mean = where, sd = length(na.omit(x))/10))
        newlocation <- abs(rnorm(1, mean = where, sd = sum(outvec>1)/10))
        if(newlocation > length(na.omit(x))) {newlocation <- where}
        if(newlocation < 1) {newlocation <- where}
        newstart <- x[round(newlocation)]
        if(newstart < start) {start <- newstart}
        start<<-start
      }
      startvector <- replicate(10000, findmin())
      decision <- which(x==start)
    }
    
    q <- seq(1, length(na.omit(x)), length.out = log(length(na.omit(x))))
    p<-x[q]
    
    guesses <<- sapply(p, scanformin)
    if(length(guesses) == 0) {guesses <- 1}
    guessesuniquesorted <- sort(unique(c(guesses)))
    guessesuniquesorted <- c(guessesuniquesorted, length(na.omit(x)))
    
    
    # xdif <- c(log(x[1]), log(x))
    # xdif <- xdif[1:length(xdif)-1]
    # xsub <- (log(x) - xdif)
    # greatestdif <- ifelse(max(xsub) > 0.6931472, which(xsub == max(xsub))-1, max(guessesuniquesorted)) 
    
 
    if(is.null(greatestdif)) {greatestdif <- length(na.omit(x))}
    guessesuniquesorted <- sort(unique(c(guessesuniquesorted, greatestdif)))
    guessesuniquesorted <- guessesuniquesorted[guessesuniquesorted!=0]
    guessesuniquesortedstaggered <- c(-10000000, guessesuniquesorted)
    difference <- c(guessesuniquesorted, 10000000) - guessesuniquesortedstaggered
   
     if (min(difference) == 1) {
      breaktoremove <- max(which(difference == 1)) - 1
      guessesuniquesorted <- guessesuniquesorted[-breaktoremove]
    }
    guessesuniquesorted[guessesuniquesorted > max(guessesuniquesorted)-5] <- max(guessesuniquesorted)
    guessesuniquesorted <- unique(guessesuniquesorted)
    guessesuniquesorted <<- guessesuniquesorted
    
    accountsfor <- function(t) {
      outvecsorted <- sort(outvec)
      topnumbers <- tail(outvecsorted, t)
      sum(topnumbers) / sum(outvec)
    }
    
    outvec[outvec < 0] <- 0
    fractionaccounted <- sapply(guessesuniquesorted, accountsfor)
    staggered <- c(0, fractionaccounted)[1:length(fractionaccounted)]
    subtracted <- fractionaccounted - staggered 
    

    indices <- data.frame(guessesuniquesorted, subtracted)
    colnames(indices) <- c("Number of barcodes", "Fraction of reads")
    
    
    weights <- log(indices[,2])
    values <- (indices[,1])
    weightsforsubtraction <- c(weights[2:length(values)], 0)
    weightsdif <- (weights - weightsforsubtraction)[1:length(weights)-1]
    if(length(weightsdif)==0) {weightsdif <- values}
    noisestart <- indices[,1][which(weightsdif == max(weightsdif))]
    noisestartcopy <- noisestart
    
    #if there are no populations that are below the minimum weight threshold, set noise to be the end of the last detected population
    if (is.na(indices[,1][min(which(indices[,2] < minweight))])) {noisestart <- max(indices[,1])}
    #if the sum of the weights of the population after the start of noise is greater than the miniumum weight threshold, set the start to be where the the rest of the reads after are under the minimum weight
    locationofminweightcutoff <- min(which(cumsum(indices[,2])>(1-minweight)))
    if (sum(indices[,2][which(indices[,1] > noisestart)]) > minweight) {noisestart <- indices[,1][locationofminweightcutoff]}
    
    
    

    FirstResample <- as.numeric(rmultinom(1, sum(outvec), ReferenceVector/sum(ReferenceVector)))
    steps1 <- round(seq(from = 1, to = length(FirstResample), length.out = 100))
    steps2 <- round(seq(from = length(FirstResample), to = length(FirstResample)*20, length.out = 100))
    steps <- c(steps1, steps2)
    
    GetNewBotTable <- function(n) {
      #vec <- as.numeric(rmvhyper(1, FirstResample, n))
      vec <- rmultinom(5, n, FirstResample/sum(FirstResample))
      vec[vec!=0] <- 1
      mean(colSums(vec))
    }
    y <- as.numeric(sapply(steps, GetNewBotTable))
    
    interpol <- approx(x = steps, y = y, n = length(invec))
    xvals <- as.numeric(unlist(interpol[1]))
    yvals <- as.numeric(unlist(interpol[2]))
    
    dfxy2 <- sortedXyData(x = xvals, y = yvals)
    Ns <- NLSstClosestX(dfxy2, noisestart)
    
    inputprop <- na.omit(ReferenceVector/sum(ReferenceVector))
    outprop <- na.omit(outvec / sum(outvec))
    num <- ( outprop - inputprop ) ^ 2
    den <- inputprop*(1-inputprop)
    sigma <- num / den
    sigma <- sigma[sigma!=Inf]
    F <- mean(na.omit(sigma))
    Nb <- 1 / (F - 1/sum(invec) - 1/sum(outvec)) 
    
    if(plots){
      
      converttocutoffs<- function(p) {
        cutoffposition <- length(outvec) - p
        cutoffvalue <- bindsorted[cutoffposition+1,2]
        if(cutoffvalue == 0) {cutoffvalue <- 1}    
        cutoffvalue/sum(outvec) }
      cutoffpositions <- unlist(lapply(guessesuniquesorted, converttocutoffs))
      cutoffpositions <<- cutoffpositions
      
      par(mfrow = c(2,1))
      plot(1:times,x, log = "y", ylim = c(.01, 2E6), xlim = c(1, length(as.numeric(na.omit(x)))), main = "Resiliency", ylab = "Nb", xlab = "Iteration")
      abline(v=guessesuniquesorted)
      plot(outvec/sum(outvec), log = "y", ylim = c(1E-8, 1), main = "Barcodes", ylab = "Frequency", xlab = "Barcode")
      abline(h = cutoffpositions)
      print(indices)
      print(paste("Nb=", Nb, sep = ""))
      print(paste("Ns=", Ns, sep = ""))
      
      Nb <<- Nb
      Ns <<- Ns
      indices <<- indices
      outvec <<- outvec
      invec <<- invec
      dfxy2 <<- dfxy2
      FirstResample <<- FirstResample
      noisestart <<- noisestart
      x<<-x
      oneratio <<- oneratio
    }
    
    
    if(plots == FALSE) {
      print(samplename)
      
      numberofzeros <- length(invec) - noisestart
      outvec[order(outvec)][1:numberofzeros] <- 0
      
      TableWithoutNoise <- data.frame(TableWithoutNoise, outvec)
      colnames(TableWithoutNoise)[length(colnames(TableWithoutNoise))] <- samplename
      TableWithoutNoise <<- TableWithoutNoise
      outvec <<- outvec
      invec <<- ReferenceVector
      c(TotalReads, noisestart, Nb, Ns )
    }
  }
  

  SampleNames <- colnames(ReadsTable)[-WhereAreReferences]
  TableOfEstimates <- t(sapply(SampleNames, ResiliencyIndices))
  
  colnames(TableOfEstimates) <- c("TotalReads", "Number of barcodes", "Nb", "Ns")
  
  write.csv(TableOfEstimates, outputfilename)
  write.csv(TableWithoutNoise, "FrequenciesWithoutNoise.csv")
  TableOfEstimates <<- TableOfEstimates
  FrequenciesWithoutNoise <<- TableWithoutNoise
  ReadsTable <<- ReadsTable
  CFUtable <<- CFUtable
  ReferenceVector <<- ReferenceVector
  TestNames <<- TestNames
  minweight <<- minweight
  ReadsTableName <<- ReadsTableName
}
